# -*- coding: utf-8 -*-
"""final_finetuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bbbqdqk7a8d3qjIgPc0VhJFp9tpj042p
"""

# =========================================================
# üß† Fine-tuning Small Language Model with Teacher LLM
# by Pongsatron / Template for AIAT Internship Project
# =========================================================

# ‚úÖ STEP 1: Install dependencies
!pip install -q transformers peft datasets accelerate bitsandbytes openai

# ‚úÖ STEP 2: Import libraries
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from openai import OpenAI
import pandas as pd
import os, json, time, random
from google.colab import userdata

OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')

# üîë ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ API key ‡∏Ç‡∏≠‡∏á OpenRouter (‡∏´‡∏£‡∏∑‡∏≠ OpenAI)
client = OpenAI(
  base_url="https://openrouter.ai/api/v1",
  api_key=OPENROUTER_API_KEY,
)

# ====== CONFIG ======
MAIN_PATH = "/content/drive/MyDrive/demo_finetuning/"
CSV_PATH = MAIN_PATH + "teacher_prompts_50.csv"          # ‡πÑ‡∏ü‡∏•‡πå prompt ‡∏ä‡∏∏‡∏î 200 ‡πÅ‡∏ñ‡∏ß
DDI_CSV  = MAIN_PATH + "db_drug_interactions.csv"         # ‡πÑ‡∏ü‡∏•‡πå drug-drug interactions (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)

CSV_JSON = MAIN_PATH + "teacher_prompts_50.json"          # ‡πÑ‡∏ü‡∏•‡πå prompt ‡∏ä‡∏∏‡∏î 200 ‡πÅ‡∏ñ‡∏ß
DDI_JSON = MAIN_PATH + "db_drug_interactions.json"

MODEL_NAME = "openai/gpt-4o-mini"       # teacher model
OUT_CSV   = MAIN_PATH + "teacher_dataset.csv"             # dataset ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô CSV
OUT_JSONL = MAIN_PATH + "teacher_dataset.jsonl"           # dataset ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô JSONL

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏¥‡∏ö (‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ)
INCLUDE_RAW_DB_TASKS = True

# ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î / backoff
SLEEP_BETWEEN_CALLS = 1.0
MAX_RETRIES = 3
BACKOFF_BASE = 1.8

from google.colab import drive
drive.mount('/content/drive')

# ====== TEST CONNECTION (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°) ======
test = client.responses.create(model=MODEL_NAME, input="ping")
print("‚úÖ OpenRouter connected:", test.output_text[:60], "...")

# ====== HELPERS ======
def robust_input_text(instruction: str, input_payload: str | None) -> str:
    """
    ‡∏£‡∏ß‡∏° instruction + input ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏î‡∏µ‡πà‡∏¢‡∏ß
    """
    input_payload = (input_payload or "").strip()
    if input_payload:
        return f"{instruction.strip()}\n\nINPUT:\n{input_payload}"
    return instruction.strip()

def llm_call_with_retry(model: str, prompt: str):
    """
    ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM ‡∏û‡∏£‡πâ‡∏≠‡∏° retry + backoff ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
    """
    last_err = None
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            resp = client.responses.create(model=model, input=prompt)
            return resp.output_text
        except Exception as e:
            last_err = e
            wait = (BACKOFF_BASE ** (attempt - 1)) + random.uniform(0, 0.3)
            print(f"‚ö†Ô∏è  Error (attempt {attempt}/{MAX_RETRIES}): {e} ‚Üí retry in {wait:.1f}s")
            time.sleep(wait)
    # ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡πÄ‡∏•‡∏¢ ‡πÉ‡∏´‡πâ‡πÇ‡∏¢‡∏ô error ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
    raise last_err

def append_record(dataset: list, instruction: str, input_payload: str, output_text: str, meta: dict):
    """
    ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏ñ‡∏ß‡∏•‡∏á dataset (‡πÄ‡∏Å‡πá‡∏ö instruction/input/output + metadata)
    """
    row = {
        "instruction": instruction,
        "input": input_payload or "",
        "output": output_text,
    }
    # ‡πÅ‡∏ô‡∏ö metadata ‡πÑ‡∏ß‡πâ‡πÉ‡∏´‡πâ
    for k, v in (meta or {}).items():
        row[k] = v
    dataset.append(row)

  # ====== (‡∏ó‡∏≤‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏á‡∏≤‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô Medicine + DDI ======
def safe_get(row, key):
    try:
        v = row.get(key, "")
        if pd.isna(v):
            return ""
        return str(v)
    except Exception:
        return ""

# ====== ‡πÇ‡∏´‡∏•‡∏î prompt ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå teacher_prompts_50.csv ======
if not os.path.exists(CSV_PATH):
    raise FileNotFoundError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå {CSV_PATH} ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô")

prompts_df = pd.read_csv(CSV_PATH)
required_cols = {"instruction", "input"}
missing = required_cols - set(map(str.lower, prompts_df.columns))
# ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏Å‡∏£‡∏ì‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÉ‡∏´‡∏ç‡πà/‡πÄ‡∏•‡πá‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô
colmap = {c.lower(): c for c in prompts_df.columns}
instruction_col = colmap.get("instruction")
input_col = colmap.get("input")

if instruction_col is None or input_col is None:
    raise ValueError(f"‡πÑ‡∏ü‡∏•‡πå {CSV_PATH} ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå instruction ‡πÅ‡∏•‡∏∞ input")

# ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡πÄ‡∏°‡∏ï‡∏≤ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
task_type_col  = colmap.get("task_type")
language_col   = colmap.get("language")
difficulty_col = colmap.get("difficulty")
expected_col   = colmap.get("expected_output_format")

dataset = []
ddi_dataset = []

print(f"‚ñ∂Ô∏è  Generating from {CSV_PATH} ({len(prompts_df)} rows) ...")
if not os.path.exists(CSV_JSON):
  for i, row in prompts_df.iterrows():
      try:
          instruction = str(row[instruction_col] or "").strip()
          input_payload = "" if pd.isna(row[input_col]) else str(row[input_col]).strip()

          # ‡∏£‡∏ß‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ï‡∏±‡∏ß‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏Ñ‡πâ‡∏î‡πÄ‡∏î‡∏¥‡∏°‡πÉ‡∏ä‡πâ param 'input')
          prompt_text = robust_input_text(instruction, input_payload)

          # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å teacher
          output_text = llm_call_with_retry(MODEL_NAME, prompt_text)

          # ‡πÅ‡∏ô‡∏ö‡πÄ‡∏°‡∏ï‡∏≤
          meta = {}
          if task_type_col:  meta["task_type"] = row.get(task_type_col, "")
          if language_col:   meta["language"] = row.get(language_col, "")
          if difficulty_col: meta["difficulty"] = row.get(difficulty_col, "")
          if expected_col:   meta["expected_output_format"] = row.get(expected_col, "")

          append_record(dataset, instruction, input_payload, output_text, meta)

          if (i + 1) % 10 == 0:
              print(f"  ‚úì Done {i+1}/{len(prompts_df)}")
          time.sleep(SLEEP_BETWEEN_CALLS)
      except Exception as e:
          print(f"‚ùå Error at row {i+1}: {e}")
          continue
  export_json = json.dumps(dataset, ensure_ascii=False)
  with open(CSV_JSON, "w", encoding="utf-8") as f:
      f.write(export_json)
else:
  with open(CSV_JSON, "r", encoding="utf-8") as f:
      dataset = json.load(f)


if INCLUDE_RAW_DB_TASKS:
    # ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
    ddi_df = pd.read_csv(DDI_CSV) if os.path.exists(DDI_CSV) else pd.DataFrame()


    # ------------ ‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å db_drug_interactions ------------
    if not ddi_df.empty:
        def fcol(df, names, default=None):
            low = {c.lower(): c for c in df.columns}
            for n in names:
                for c in low:
                    if n.lower() == c or n.lower() in c:
                        return low[c]
            return default or df.columns[0]

        a_col = fcol(ddi_df, ["drug_a","drug1","object","perpetrator","index_drug"])
        b_col = fcol(ddi_df, ["drug_b","drug2","precipitant","victim","coadministered"])
        sev_col = fcol(ddi_df, ["severity","risk_level","level"])
        mech_col = fcol(ddi_df, ["mechanism","moa","mechanism_of_interaction"])
        mgmt_col = fcol(ddi_df, ["management","action","recommendation","clinical_management"])
        desc_col = fcol(ddi_df, ["description","details","interaction_description","notes"])

        print(f"‚ñ∂Ô∏è  Generating from {DDI_CSV} ({len(ddi_df)} rows) ...")

        if not os.path.exists(DDI_JSON):
          for i, row in ddi_df.iterrows():
              try:
                  # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏á‡∏≤‡∏ô: DDI explanation (EN)
                  instruction = f"Explain the drug‚Äìdrug interaction between {safe_get(row, a_col)} and {safe_get(row, b_col)}. Include severity, mechanism, clinical consequences, and recommended management."
                  input_payload = json.dumps({
                      "severity": safe_get(row, sev_col),
                      "mechanism": safe_get(row, mech_col),
                      "description": safe_get(row, desc_col),
                      "management": safe_get(row, mgmt_col),
                  }, ensure_ascii=False)

                  prompt_text = robust_input_text(instruction, input_payload)
                  output_text = llm_call_with_retry(MODEL_NAME, prompt_text)

                  meta = {
                      "task_type": "ddi_explain.en(raw)",
                      "language": "en",
                      "source": "db_drug_interactions.csv"
                  }
                  append_record(ddi_dataset, instruction, input_payload, output_text, meta)
                  if (i + 1) % 50 == 0:
                      print(f"  ‚úì DDI {i+1}/{len(ddi_df)}")
                  time.sleep(SLEEP_BETWEEN_CALLS)
              except Exception as e:
                  print(f"‚ùå DDI row {i+1}: {e}")
                  continue
          export_json = json.dumps(ddi_dataset, ensure_ascii=False)
          with open(DDI_JSON, "w", encoding="utf-8") as f:
              f.write(export_json)
        else:
          with open(DDI_JSON, "r", encoding="utf-8") as f:
              ddi_dataset = json.load(f)

dataset = [*dataset, *ddi_dataset]

# ====== ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå ======
df = pd.DataFrame(dataset)
df.to_csv(OUT_CSV, index=False)
print(f"‚úÖ Dataset generated with {len(df)} samples and saved as {OUT_CSV}")

with open(OUT_JSONL, "w", encoding="utf-8") as f:
    for _, r in df.iterrows():
        f.write(json.dumps({
            "instruction": r["instruction"],
            "input": r.get("input", ""),
            "output": r["output"],
            # ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏°‡∏ï‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            "meta": {
                "task_type": r.get("task_type",""),
                "language": r.get("language",""),
                "difficulty": r.get("difficulty",""),
                "expected_output_format": r.get("expected_output_format",""),
                "source": r.get("source","teacher_prompts_50.csv")
            }
        }, ensure_ascii=False) + "\n")
print(f"‚úÖ Also saved JSONL at {OUT_JSONL}")

# -*- coding: utf-8 -*-
import os, time, functools, math, random, json
import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling, BitsAndBytesConfig, EarlyStoppingCallback,
    set_seed
)
# Import ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LoRA
from peft import LoraConfig, get_peft_model, PeftModel

# =========================
# üîß CONFIG
# =========================
SEED = 42
VAL_SIZE = 0.1                  # 10% validation
PATIENCE = 2                    # early stopping patience
EVAL_STRATEGY = "epoch"         # "epoch" or "steps"
MODEL_NAME = "Qwen/Qwen3-1.7B"
MAX_LENGTH = 2048
SAMPLE_N = 20 # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô sample ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö quick compare


if 'MAIN_PATH' not in globals():
    MAIN_PATH = "/content/drive/MyDrive/demo_finetuning/" # <--- ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç path ‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ

OUTPUT_DIR = MAIN_PATH + "finetuned_qwen3_1p7b_lora_checkpoints" # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏™‡∏±‡∏ö‡∏™‡∏ô‡∏Å‡∏±‡∏ö‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏ü‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
SAVE_DIR = MAIN_PATH + "qwen3_1p7b-instruct-lora" # ‡∏ó‡∏µ‡πà‡πÄ‡∏ã‡∏ü‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
DATA_CSV_PATH = MAIN_PATH + "teacher_dataset.csv" # Path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
LOG_CSV = MAIN_PATH + "training_logs.csv" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å training logs
COMPARE_CSV = MAIN_PATH + "winrate_20.csv" # Path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏• quick compare
USE_CHAT_TEMPLATE = False # ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö chat template ‡∏Ç‡∏≠‡∏á HF ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏õ‡πá‡∏ô True

os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
set_seed(SEED)

# =========================
# ‚öôÔ∏è Load tokenizer/model
# =========================
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
)

def with_retry(fn, retries=3, delay=8):
    import functools, time
    @functools.wraps(fn)
    def _wrap(*args, **kwargs):
        last = None
        for i in range(retries):
            try:
                return fn(*args, **kwargs)
            except Exception as e:
                last = e
                print(f"[retry {i+1}/{retries}] {e}")
                time.sleep(delay)
        raise last
    return _wrap

load_tokenizer = with_retry(lambda: AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True))
tokenizer = load_tokenizer()
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

load_base_model = with_retry(lambda: AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    trust_remote_code=True,
    quantization_config=bnb_config,
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,
    low_cpu_mem_usage=True
))
base_model = load_base_model()

# =================================================================
# ---  New logic: check if a model exists, if not, train. ---
# =================================================================

# Check if a trained model adapter already exists
if os.path.exists(SAVE_DIR) and 'adapter_config.json' in os.listdir(SAVE_DIR):
    # --- If model exists, load it ---
    print(f"‚úÖ Found existing fine-tuned model at '{SAVE_DIR}'. Loading it.")
    model = PeftModel.from_pretrained(base_model, SAVE_DIR)
    print("‚úÖ Model loaded successfully from saved directory.")

else:
    # --- If model does not exist, start training ---
    print(f"‚ùå No fine-tuned model found at '{SAVE_DIR}'. Starting a new training session.")

    # Create LoRA model from base for training
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
    )
    model = get_peft_model(base_model, lora_config)
    model.print_trainable_parameters()

    # Load dataset
    def load_rows_from_sources():
        df = pd.DataFrame()
        if os.path.exists(DATA_CSV_PATH):
            try:
                df = pd.read_csv(DATA_CSV_PATH)
                print(f"Loaded data from {DATA_CSV_PATH}")
            except Exception as e:
                print(f"Error loading {DATA_CSV_PATH}: {e}")
        else:
            print(f"Data file not found at {DATA_CSV_PATH}")

        # Ensure required columns exist and handle NaNs
        for col in ["instruction", "input", "output"]:
            if col not in df.columns:
                df[col] = ""
            df[col] = df[col].fillna("").astype(str)

        # Filter for samples with non-empty instruction and output
        df = df[(df["instruction"].str.strip() != "") & (df["output"].str.strip() != "")]
        df = df.drop_duplicates(subset=["instruction", "input"], keep="last").reset_index(drop=True)

        if len(df) == 0:
            raise ValueError("No trainable samples found: instruction + output are required.")
        return df

    df = load_rows_from_sources()
    print(f"‚úÖ Loaded samples: {len(df)}")
    dataset = Dataset.from_pandas(df)

    # Prompt formatting
    def format_example(example):
        instruction = (example.get("instruction") or "").strip()
        inp = (example.get("input") or "").strip()
        output = (example.get("output") or "").strip()

        if inp:
            user_text = f"### Instruction:\n{instruction}\n\nINPUT:\n{inp}"
        else:
            user_text = f"### Instruction:\n{instruction}"

        text = f"{user_text}\n\n### Response:\n{output}{tokenizer.eos_token}"
        return tokenizer(text, truncation=True, max_length=MAX_LENGTH, padding=False)

    tokenized = dataset.map(format_example, remove_columns=dataset.column_names)

    # Train/Val split
    split = tokenized.train_test_split(test_size=VAL_SIZE, seed=SEED)
    train_dataset = split["train"]
    eval_dataset  = split["test"]
    print(f"Train: {len(train_dataset)} | Val: {len(eval_dataset)}")

    # Collator
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # TrainingArguments + EarlyStopping
    use_bf16 = torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        learning_rate=2e-4,
        num_train_epochs=5,
        logging_steps=10,
        save_total_limit=1,
        save_strategy=EVAL_STRATEGY,
        evaluation_strategy=EVAL_STRATEGY, # Correct parameter name
        bf16=use_bf16,
        fp16=not use_bf16,
        report_to="none",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        seed=SEED
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]
    )

    # Train
    print("üèãÔ∏è Starting training...")
    trainer.train()

    # Save best model
    print(f"üíæ Saving best model to {SAVE_DIR}...")
    model.save_pretrained(SAVE_DIR)
    tokenizer.save_pretrained(SAVE_DIR)
    print(f"‚úÖ Fine-tuning complete! Model saved at {SAVE_DIR}")

# ===============================================================
# --- END OF NEW LOGIC ---
# At this point, the `model` variable is ready for inference,
# whether it was loaded from disk or freshly trained.
# ===============================================================
print("\nModel is ready for use.")

# -*- coding: utf-8 -*-
import os, time, functools, math, random, json
import pandas as pd
import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling, BitsAndBytesConfig, EarlyStoppingCallback,
    set_seed
)
# Import ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• LoRA
from peft import LoraConfig, get_peft_model, PeftModel

# =========================
# üìä Extract train/eval losses ‚Üí CSV
# =========================
log_hist = []
if 'trainer' in globals() and hasattr(trainer, 'state') and hasattr(trainer.state, 'log_history'):
    log_hist = trainer.state.log_history

# ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ record ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ loss
records = []
for rec in log_hist:
    row = {}
    if "loss" in rec: row["train_loss"] = rec["loss"]
    if "eval_loss" in rec: row["eval_loss"] = rec["eval_loss"]
    if "epoch" in rec: row["epoch"] = rec["epoch"]
    if row:
        records.append(row)
df_log = pd.DataFrame(records)

if not df_log.empty:
    df_log.to_csv(LOG_CSV, index=False)
    last_train_loss = df_log["train_loss"].dropna().iloc[-1] if "train_loss" in df_log and df_log["train_loss"].notna().any() else None
    last_eval_loss  = df_log["eval_loss"].dropna().iloc[-1]  if "eval_loss"  in df_log and df_log["eval_loss"].notna().any()  else None
    print(f"üìâ Final train loss: {last_train_loss}")
    print(f"üß™ Final eval  loss: {last_eval_loss}")
    print(f"üìù Logs saved: {LOG_CSV}")
else:
    last_train_loss = None
    last_eval_loss = None
    print("‚ö†Ô∏è No training logs found. Skipping loss extraction.")


# =========================
# üß™ Quick before/after compare (win-rate@{SAMPLE_N})
# =========================
def rouge_l_f(ref, hyp):
    ref_tokens = ref.split()
    hyp_tokens = hyp.split()
    m, n = len(ref_tokens), len(hyp_tokens)
    dp = [[0]*(n+1) for _ in range(m+1)]
    for i in range(m):
        for j in range(n):
            if ref_tokens[i] == hyp_tokens[j]:
                dp[i+1][j+1] = dp[i][j] + 1
            else:
                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])
    lcs = dp[m][n]
    prec = lcs / (n or 1)
    rec  = lcs / (m or 1)
    if prec + rec == 0: return 0.0
    return 2 * prec * rec / (prec + rec)

# ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå CSV ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏• win-rate
RESULT_DIR = "/content/drive/MyDrive/demo_finetuning"
os.makedirs(RESULT_DIR, exist_ok=True)
WINRATE_CSV = os.path.join(RESULT_DIR, "winrate_20.csv")

# üîé ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå winrate ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÑ‡∏´‡∏°
if os.path.exists(WINRATE_CSV):
    print(f"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {WINRATE_CSV}")
    df_cmp = pd.read_csv(WINRATE_CSV)
    win_rate = (df_cmp["winner"] == "FT").mean() if "winner" in df_cmp.columns else 0.0

    print("\n========== üìä PREVIOUS EVALUATION SUMMARY ==========")
    print(f"Train loss (last): {last_train_loss}")
    print(f"Eval  loss (last): {last_eval_loss}")
    try:
        ppl = math.exp(float(last_eval_loss)) if last_eval_loss is not None and pd.notnull(last_eval_loss) else None
        print(f"Perplexity (eval): {ppl:.2f}" if ppl is not None else "Perplexity (eval): N/A")
    except Exception:
        print("Perplexity (eval): Calculation Error")

    print(f"Win-rate@{len(df_cmp)} (FT vs BASE): {win_rate*100:.1f}%")
    print("üëÄ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å:")
    print(df_cmp.head().to_string(index=False))
    print("====================================================")

else:
    print("üîé ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå winrate_20.csv ‚Äî ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÉ‡∏´‡∏°‡πà...")

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 20 ‡πÅ‡∏ñ‡∏ß‡∏à‡∏≤‡∏Å‡∏ä‡∏∏‡∏î eval ‡∏î‡∏¥‡∏ö
    if 'eval_dataset' in globals() and "__index_level_0__" in eval_dataset.column_names:
        eval_raw = df.iloc[eval_dataset["__index_level_0__"]]
    else:
        eval_raw = df.sample(min(SAMPLE_N, len(df)), random_state=SEED)

    eval_sample = eval_raw.sample(min(SAMPLE_N, len(eval_raw)), random_state=SEED).reset_index(drop=True)

    def build_user_text_from_df_row(r):
        instr = (r.get("instruction","") or "").strip()
        inp = (r.get("input","") or "").strip()
        return f"{instr}\n\nINPUT:\n{inp}" if inp else instr

    @torch.no_grad()
    def generate_batch(model_obj, rows_df, max_new_tokens=196):
        model_obj.eval()
        outs = []
        for _, r in rows_df.iterrows():
            user_text = build_user_text_from_df_row(r)
            if USE_CHAT_TEMPLATE:
                messages = [{"role": "user", "content": user_text}]
                prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            else:
                prompt_text = f"### Instruction:\n{user_text}\n\n### Response:\n"
            inputs = tokenizer(prompt_text, return_tensors="pt").to(model_obj.device)
            gen = model_obj.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                eos_token_id=tokenizer.eos_token_id
            )
            decoded = tokenizer.decode(gen[0], skip_special_tokens=True)
            prompt_end_index = decoded.find(prompt_text)
            outs.append(decoded[prompt_end_index + len(prompt_text):].strip() if prompt_end_index != -1 else decoded.strip())
        return outs

    print("üîé Generating base vs finetuned outputs on 20 samples...")
    base_for_eval = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        trust_remote_code=True,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else None,
        low_cpu_mem_usage=True
    )

    if hasattr(base_for_eval, "gradient_checkpointing_enable"):
        base_for_eval.gradient_checkpointing_enable()

    base_preds = generate_batch(base_for_eval, eval_sample)
    ft_preds   = generate_batch(model,         eval_sample)

    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE-L ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•
    rows_cmp, wins = [], 0
    for i, r in eval_sample.iterrows():
        ref = (r.get("output","") or "")
        b = rouge_l_f(ref, base_preds[i])
        f = rouge_l_f(ref, ft_preds[i])
        winner = "FT" if f > b else ("BASE" if b > f else "TIE")
        if winner == "FT": wins += 1
        rows_cmp.append({
            "idx": i,
            "instruction": (r["instruction"][:120] + "‚Ä¶") if len(r["instruction"]) > 120 else r["instruction"],
            "base_score_rougeL": round(b, 4),
            "ft_score_rougeL": round(f, 4),
            "winner": winner
        })

    df_cmp = pd.DataFrame(rows_cmp)
    df_cmp.to_csv(WINRATE_CSV, index=False)
    win_rate = wins / len(df_cmp) if len(df_cmp) else 0.0

    print("========== üìä EVALUATION SUMMARY ==========")
    print(f"Train loss (last): {last_train_loss}")
    print(f"Eval  loss (last): {last_eval_loss}")
    try:
        ppl = math.exp(float(last_eval_loss)) if last_eval_loss is not None and pd.notnull(last_eval_loss) else None
        print(f"Perplexity (eval): {ppl:.2f}" if ppl is not None else "Perplexity (eval): N/A")
    except Exception:
        print("Perplexity (eval): Calculation Error")

    print(f"Win-rate@{len(df_cmp)} (FT vs BASE): {win_rate*100:.1f}%")
    print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏á: {WINRATE_CSV}")
    print("==========================================")

!pip install rouge_score

!pip install bert-score

# -*- coding: utf-8 -*-
import os, glob, math, re, datetime as dt
from collections import Counter
import pandas as pd
import numpy as np
import torch

# ==============================================================================
# ‡∏™‡∏Ñ‡∏£‡∏¥‡∏õ‡∏ï‡πå‡∏ô‡∏µ‡πâ‡∏à‡∏∞:
# - ‡πÇ‡∏´‡∏•‡∏î df_val ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ (‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 ‡πÅ‡∏ñ‡∏ß)
# - ‡πÉ‡∏ä‡πâ build_prompt ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ
# - ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏û‡∏∂‡πà‡∏á trainer) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏ï‡πà‡∏≤‡∏á ‡πÜ
# - ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡πÄ‡∏õ‡πá‡∏ô CSV /content/drive/MyDrive/demo_finetuning/metrics_val_*.csv
# - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤: ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÅ‡∏ó‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà (‡∏ï‡∏±‡πâ‡∏á FORCE_RERUN=True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô)
# ==============================================================================

def _has(name):
    return (name in globals()) and (globals()[name] is not None)

# ====== 0) ‡πÇ‡∏´‡∏•‡∏î df_val ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ ======
def _pick_latest(paths):
    paths = [p for p in paths if os.path.exists(p)]
    if not paths: return None
    return max(paths, key=lambda p: os.path.getmtime(p))

if not _has("df_val"):
    CANDIDATES = []
    base_dirs = [".", "/content", "/content/drive/MyDrive", "/content/drive/MyDrive/demo_finetuning"]
    names = ["val.csv", "validation.csv", "df_val.csv", "valid.csv", "dev.csv",
             "val.parquet", "validation.parquet"]
    for d in base_dirs:
        for n in names:
            CANDIDATES.append(os.path.join(d, n))

    hit = _pick_latest([p for p in CANDIDATES if p.endswith(".csv")]) \
          or _pick_latest([p for p in CANDIDATES if p.endswith(".parquet")])

    if hit:
        try:
            if hit.endswith(".csv"):
                df_val = pd.read_csv(hit)
            else:
                df_val = pd.read_parquet(hit)
            print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î df_val ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {hit} | shape={df_val.shape}")
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå {hit} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

if not _has("df_val"):
    default_csv = "/content/drive/MyDrive/demo_finetuning/validation.csv"
    if os.path.exists(default_csv):
        try:
            df_val = pd.read_csv(default_csv)
            print(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î df_val ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå: {default_csv} | shape={df_val.shape}")
        except Exception as e:
            print(f"‚ö†Ô∏è ‡πÇ‡∏´‡∏•‡∏î {default_csv} ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

if not _has("df_val"):
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå validation ‚Äî ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á df_val ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á 3 ‡πÅ‡∏ñ‡∏ß (‡πÇ‡∏õ‡∏£‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì)")
    df_val = pd.DataFrame([
        {"input":"Explain the mechanism of action of aspirin in one sentence.",
         "output":"Aspirin irreversibly inhibits COX-1 and COX-2 to reduce prostaglandin and thromboxane synthesis.",
         "drug":"aspirin"},
        {"input":"Describe the MOA of ibuprofen in one sentence.",
         "output":"Ibuprofen reversibly inhibits COX-1 and COX-2 to decrease prostaglandin production.",
         "drug":"ibuprofen"},
        {"input":"What is the mechanism of action of omeprazole? (one sentence)",
         "output":"Omeprazole irreversibly inhibits the gastric H+/K+-ATPase proton pump to suppress acid secretion.",
         "drug":"omeprazole"},
        {"input":"Give the MOA of loratadine in one sentence.",
         "output":"Loratadine antagonizes H1 histamine receptors to reduce allergic symptoms.",
         "drug":"loratadine"},
        {"input":"Explain how fluoxetine works (one sentence).",
         "output":"Fluoxetine inhibits SERT to increase synaptic serotonin levels.",
         "drug":"fluoxetine"},
        {"input":"Describe sertraline's mechanism of action in one sentence.",
         "output":"Sertraline selectively inhibits SERT to block serotonin reuptake.",
         "drug":"sertraline"},
        {"input":"What is haloperidol's mechanism of action? (one sentence)",
         "output":"Haloperidol antagonizes D2 dopamine receptors to reduce positive psychotic symptoms.",
         "drug":"haloperidol"},
        {"input":"Explain diazepam's mechanism of action in one sentence.",
         "output":"Diazepam positively modulates GABA-A receptors to enhance inhibitory GABAergic transmission.",
         "drug":"diazepam"},
        {"input":"Give the MOA of ketamine in one sentence.",
         "output":"Ketamine noncompetitively antagonizes NMDA receptors to reduce excitatory neurotransmission.",
         "drug":"ketamine"},
        {"input":"Explain the MOA of salbutamol (albuterol) in one sentence.",
         "output":"Salbutamol is a Œ≤2 agonist that activates adenylate cyclase, increases cAMP, and relaxes bronchial smooth muscle.",
         "drug":"salbutamol"},
        {"input":"Describe propranolol's mechanism of action in one sentence.",
         "output":"Propranolol antagonizes Œ≤1 and Œ≤2 adrenergic receptors to decrease heart rate and contractility.",
         "drug":"propranolol"},
        {"input":"What is losartan's mechanism of action? (one sentence)",
         "output":"Losartan is an ARB that antagonizes AT1 receptors to block angiotensin II effects.",
         "drug":"losartan"},
        {"input":"Explain the MOA of lisinopril in one sentence.",
         "output":"Lisinopril inhibits ACE to reduce angiotensin II formation and bradykinin breakdown.",
         "drug":"lisinopril"},
        {"input":"State amlodipine's mechanism of action in one sentence.",
         "output":"Amlodipine blocks L-type Ca2+ channels in vascular smooth muscle causing vasodilation.",
         "drug":"amlodipine"},
        {"input":"Explain lidocaine's mechanism of action in one sentence.",
         "output":"Lidocaine blocks voltage-gated Na+ channels to prevent action potential propagation.",
         "drug":"lidocaine"},
        {"input":"Describe clopidogrel's MOA in one sentence.",
         "output":"Clopidogrel irreversibly antagonizes platelet P2Y12 receptors to inhibit ADP-mediated aggregation.",
         "drug":"clopidogrel"},
        {"input":"What is warfarin's mechanism of action? (one sentence)",
         "output":"Warfarin inhibits VKORC1, reducing activation of vitamin K‚Äìdependent clotting factors.",
         "drug":"warfarin"},
        {"input":"Explain metformin's mechanism of action in one sentence.",
         "output":"Metformin activates AMPK to decrease hepatic gluconeogenesis and improve insulin sensitivity.",
         "drug":"metformin"},
        {"input":"State tocilizumab's mechanism of action in one sentence.",
         "output":"Tocilizumab antagonizes the IL-6 receptor and downregulates JAK/STAT signaling to reduce inflammation.",
         "drug":"tocilizumab"},
        {"input":"Give the MOA of amoxicillin in one sentence.",
         "output":"Amoxicillin inhibits bacterial cell wall synthesis by binding penicillin-binding proteins.",
         "drug":"amoxicillin"},
    ])

# ====== 1) build_prompt ‡∏™‡∏≥‡∏£‡∏≠‡∏á ======
if not _has("build_prompt"):
    def build_prompt(row):
        return str(row.get("instruction") or row.get("input") or row.get("question") or "").strip()
    print("‚ÑπÔ∏è ‡πÉ‡∏ä‡πâ build_prompt ‡πÅ‡∏ö‡∏ö‡∏™‡∏≥‡∏£‡∏≠‡∏á (‡∏î‡∏∂‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå instruction/input/question)")

# ====== 2) Evaluate & Save (robust ‡∏ï‡πà‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ trainer) ======
eval_loss, eval_ppl = float("nan"), float("nan")

if _has("trainer") and hasattr(trainer, "evaluate"):
    try:
        eval_out = trainer.evaluate()
        eval_loss = float(eval_out.get("eval_loss", float("nan")))
        eval_ppl = math.exp(eval_loss) if eval_loss == eval_loss else float("nan")
        print(f"üìä Eval loss: {eval_loss:.4f} | Perplexity: {eval_ppl:.2f}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° evaluate ‡∏Ç‡∏≠‡∏á trainer ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å: {e}")
else:
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ 'trainer' ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÄ‡∏°‡∏ò‡∏≠‡∏î evaluate ‚Äî ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô eval_loss/perplexity")

save_dir =  MAIN_PATH + "qwen3_1p7b-instruct-lora-best"
_saved_any = False

if _has("trainer") and hasattr(trainer, "save_model"):
    try:
        trainer.save_model(save_dir)
        _saved_any = True
        print(f"‚úÖ Saved best model (via trainer) at {save_dir}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡πà‡∏≤‡∏ô trainer ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

if _has("model") and hasattr(model, "save_pretrained"):
    try:
        model.save_pretrained(save_dir)
        _saved_any = True
        print(f"‚úÖ Saved model (save_pretrained) at {save_dir}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ save_pretrained ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

if _has("tokenizer") and hasattr(tokenizer, "save_pretrained"):
    try:
        tokenizer.save_pretrained(save_dir)
        _saved_any = True
        print(f"‚úÖ Saved tokenizer at {save_dir}")
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å tokenizer ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")

if not _saved_any:
    print("‚ö†Ô∏è ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏≠‡∏∞‡πÑ‡∏£‡πÄ‡∏•‡∏¢ (‡πÑ‡∏°‡πà‡∏°‡∏µ trainer/model/tokenizer ‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏î‡πâ) ‚Äî ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠")

# ====== 3) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° libs ‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å ======
try:
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
except Exception:
    sentence_bleu, SmoothingFunction = None, None

try:
    from rouge_score import rouge_scorer
except Exception:
    rouge_scorer = None

try:
    from bert_score import score as bertscore
except Exception:
    bertscore = None
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÇ‡∏°‡∏î‡∏π‡∏• 'bert-score' ‚Äî ‡∏Ç‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore (F1). ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡∏î‡πâ‡∏ß‡∏¢: pip install bert-score")

# ====== 4) ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏ï‡πà‡∏≤‡∏á ‡πÜ ======
TARGET_LEXICON = [
    r"\bCOX-?1\b", r"\bCOX-?2\b", r"\bACE\b", r"\bACE2\b", r"\bARB\b", r"\bSERT\b", r"\bNET\b",
    r"\bDAT\b", r"\bGABA[A-B]?\b", r"\bNMDA\b", r"\bH1\b", r"\bH2\b", r"\bD2\b", r"\b5-HT\d?\b",
    r"\bNa\+? channel\b", r"\bK\+? channel\b", r"\bCa\+{1,2}\b", r"\bJAK/STAT\b", r"\bMAPK\b",
]
ACTION_LEXICON = [
    r"\binhibit(s|ion|or)?\b", r"\bblock(s|er|ade)?\b", r"\bantagon(ist|ize|ism)\b",
    r"\bagon(ist|ize|ism)\b", r"\bpartial agonist\b", r"\bmodulat(e|or|ion)\b",
    r"\bstimulat(e|ion|or)\b", r"\bactivate(s|ion|d)?\b", r"\birreversible\b", r"\breversible\b"
]
PATHWAY_LEXICON = [r"\bcAMP\b", r"\bPI3K\b", r"\bJAK/STAT\b", r"\bMAPK\b", r"\bPLC\b", r"\bPKC\b"]

def extract_facets(text: str):
    text = text or ""
    def find_any(patterns):
        found = set()
        for p in patterns:
            for m in re.finditer(p, text, flags=re.IGNORECASE):
                found.add(m.group(0).lower())
        return found
    return {
        "target": find_any(TARGET_LEXICON),
        "action": find_any(ACTION_LEXICON),
        "pathway": find_any(PATHWAY_LEXICON),
    }

def f1_from_sets(true_set, pred_set):
    tp = len(true_set & pred_set)
    fp = len(pred_set - true_set)
    fn = len(true_set - pred_set)
    prec = tp / (tp + fp) if (tp + fp) else 0.0
    rec  = tp / (tp + fn) if (tp + fn) else 0.0
    f1   = 2*prec*rec / (prec+rec) if (prec+rec) else 0.0
    return f1, prec, rec

def moa_facet_f1(reference: str, prediction: str):
    ref = extract_facets(reference)
    hyp = extract_facets(prediction)
    scores = {}
    for k in ["target","action","pathway"]:
        f1, p, r = f1_from_sets(ref[k], hyp[k])
        scores[f"{k}_f1"] = f1
        scores[f"{k}_prec"] = p
        scores[f"{k}_rec"] = r
    scores["facet_f1_macro"] = np.mean([scores["target_f1"], scores["action_f1"], scores["pathway_f1"]])
    return scores

KEY_TERMS = [
    "irreversible","reversible","cox-1","cox-2","ace","sert","net","nmda","gaba","h1","h2","d2",
    "agonist","antagonist","partial agonist","inhibitor","blocker","pathway"
]
def key_term_accuracy(reference: str, prediction: str):
    ref_l = reference.lower() if reference else ""
    hyp_l = prediction.lower() if prediction else ""
    present = [k for k in KEY_TERMS if k in ref_l]
    if not present:
        return {"key_terms_covered": 1.0, "n_terms": 0}
    covered = sum(1 for k in present if k in hyp_l)
    return {"key_terms_covered": covered/len(present), "n_terms": len(present)}

ACTION_VERBS = ["inhibits","blocks","antagonizes","activates","stimulates","modulates","binds"]
def extract_triples(drug_name: str, text: str):
    text = text or ""
    triples = set()
    for verb in ACTION_VERBS:
        pat = re.compile(rf"\b({re.escape(drug_name)})\b.*?\b({verb})\b.*?\b([A-Za-z0-9\-\+\/]+)\b", re.IGNORECASE)
        for m in pat.finditer(text):
            triples.add((m.group(1).lower(), m.group(2).lower(), m.group(3).lower()))
    return triples

def triple_match(drug_name: str, ref: str, hyp: str):
    ref_t = extract_triples(drug_name, ref)
    hyp_t = extract_triples(drug_name, hyp)
    exact = len(ref_t & hyp_t)
    ref_soft = {(a,t) for (_,a,t) in ref_t}
    hyp_soft = {(a,t) for (_,a,t) in hyp_t}
    soft = len(ref_soft & hyp_soft)
    denom = max(1, len(ref_t))
    return {"triple_exact": exact/denom, "triple_soft": soft/denom, "n_ref_triples": len(ref_t)}

def bleu_score(ref, hyp):
    if sentence_bleu is None: return None
    smoothie = SmoothingFunction().method3
    ref_tok, hyp_tok = ref.split(), hyp.split()
    return sentence_bleu([ref_tok], hyp_tok, smoothing_function=smoothie)

def rougeL_score(ref, hyp):
    if rouge_scorer is None: return None
    sc = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True).score(ref, hyp)
    return sc["rougeL"].fmeasure

# ====== 5) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° df_val ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'output' ======
print(f"‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÉ‡∏ô df_val: {df_val.columns.to_list()}")
ACTUAL_ANSWER_COLUMN = 'expected_output_format'  # ‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö dataset ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
if 'output' not in df_val.columns:
    if ACTUAL_ANSWER_COLUMN in df_val.columns:
        print(f"‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤: ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'output' ... ‚Üí ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠ '{ACTUAL_ANSWER_COLUMN}' ‡πÄ‡∏õ‡πá‡∏ô 'output'")
        df_val = df_val.rename(columns={ACTUAL_ANSWER_COLUMN: 'output'})
    else:
        raise KeyError(f"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'output' ‡πÅ‡∏•‡∏∞ '{ACTUAL_ANSWER_COLUMN}' ‡πÉ‡∏ô df_val: {df_val.columns.to_list()}")
else:
    print("‚úÖ ‡∏û‡∏ö 'output' ‡πÉ‡∏ô df_val ‡πÅ‡∏•‡πâ‡∏ß ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏î‡πâ")

# ====== 6) ‡πÄ‡∏ä‡πá‡∏Ñ/‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏ã‡∏ü CSV ======
RESULT_DIR = "/content/drive/MyDrive/demo_finetuning"
os.makedirs(RESULT_DIR, exist_ok=True)
pattern = os.path.join(RESULT_DIR, "metrics_val_*.csv")
existing_csvs = sorted(glob.glob(pattern))
FORCE_RERUN = False  # True = ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏°‡πâ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤

if (len(existing_csvs) > 0) and (not FORCE_RERUN):
    latest_csv = existing_csvs[-1]
    print(f"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {latest_csv}")
    prev_df = pd.read_csv(latest_csv)

    def _safe_mean(series):
        s = pd.Series([x for x in series if pd.notnull(x)])
        return float(s.mean()) if len(s) else float("nan")

    print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤:")
    for name in ["bleu","rougeL","facet_f1_macro","key_terms_covered","triple_exact","triple_soft","bertscore_f1"]:
        if name in prev_df.columns:
            print(f"- {name}: {_safe_mean(prev_df[name]):.4f}")
        else:
            print(f"- {name}: (n/a)")
    print("\nüëÄ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å ‡πÜ:")
    print(prev_df.head().to_string(index=False))

else:
    print("üîÅ ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏Å‡πà‡∏≤‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà ‚Äî ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics ...")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ç‡∏≠‡∏á model/tokenizer/build_prompt
    if not _has("model") or not hasattr(model, "generate"):
        raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö `model` ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏° generate ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡πÄ‡∏ä‡πà‡∏ô AutoModelForCausalLM.from_pretrained)")
    if not _has("tokenizer") or not hasattr(tokenizer, "__call__"):
        raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö `tokenizer` ‚Äî ‡πÇ‡∏õ‡∏£‡∏î‡∏Å‡∏≥‡∏´‡∏ô‡∏î tokenizer ‡πÉ‡∏´‡πâ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    if not _has("build_prompt"):
        raise RuntimeError("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô `build_prompt(row)` ‚Äî ‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏û‡∏£‡∏≠‡∏°‡∏õ‡πå‡∏à‡∏≤‡∏Å df_val")

    model.eval()
    device = "cuda" if torch.cuda.is_available() else "cpu"

    def generate_answer(prompt, max_new_tokens=160):
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        with torch.no_grad():
            outs = model.generate(**inputs, max_new_tokens=max_new_tokens)
        return tokenizer.decode(outs[0], skip_special_tokens=True)

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠ batch BERTScore
    preds, refs, ids = [], [], []
    facet_rows, kta_rows, triple_rows, bleu_list, rl_list = [], [], [], [], []

    for i, row in df_val.iterrows():
        prompt = build_prompt(row)
        ref = str(row["output"])
        pred = generate_answer(prompt)

        facet = moa_facet_f1(ref, pred)
        kta   = key_term_accuracy(ref, pred)
        trip  = triple_match((row.get("drug") or row.get("DrugName") or "").strip() or "drug", ref, pred)
        bleu  = bleu_score(ref, pred)
        rL    = rougeL_score(ref, pred)

        ids.append(i)
        preds.append(pred)
        refs.append(ref)
        facet_rows.append(facet)
        kta_rows.append(kta)
        triple_rows.append(trip)
        bleu_list.append(None if bleu is None else float(bleu))
        rl_list.append(None if rL is None else float(rL) if rL is not None else None)

    # ========== BERTScore (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥) ==========
    def _thai_ratio(text):
        if not text: return 0.0
        th = sum(0x0E00 <= ord(c) <= 0x0E7F for c in text)
        return th / max(1, len(text))

    if bertscore is not None and len(preds) > 0:
        try:
            # ‡∏ñ‡πâ‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡∏Ñ‡∏£‡∏∂‡πà‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ç‡∏≠‡∏á refs ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ó‡∏¢ -> lang='th' ‡∏°‡∏¥‡∏â‡∏∞‡∏ô‡∏±‡πâ‡∏ô 'en'
            thai_refs = sum(_thai_ratio(r) > 0.3 for r in refs)
            lang_choice = 'th' if thai_refs >= (len(refs) / 2) else 'en'
            # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏™‡∏°‡∏°‡∏≤‡∏Å ‡πÜ ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏õ‡∏¥‡∏î rescale ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î bias ‡∏ê‡∏≤‡∏ô
            rescale = True if lang_choice == 'th' else False
            P, R, F1 = bertscore(preds, refs, lang=lang_choice, rescale_with_baseline=rescale)
            bs_f1 = [float(x) for x in F1.tolist()]
            print(f"‚ÑπÔ∏è BERTScore ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤: {lang_choice} | rescale_with_baseline={rescale}")
        except Exception as e:
            print(f"‚ö†Ô∏è ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BERTScore ‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
            bs_f1 = [np.nan] * len(preds)
    else:
        bs_f1 = [np.nan] * len(preds)

    # ‡∏£‡∏ß‡∏°‡πÄ‡∏õ‡πá‡∏ô DataFrame
    records = []
    for j in range(len(ids)):
        rec = {
            "id": int(ids[j]),
            "prediction": preds[j],
            "reference": refs[j],
            "bleu": bleu_list[j],
            "rougeL": rl_list[j],
            "facet_f1_macro": facet_rows[j]["facet_f1_macro"],
            "target_f1": facet_rows[j]["target_f1"],
            "action_f1": facet_rows[j]["action_f1"],
            "pathway_f1": facet_rows[j]["pathway_f1"],
            "key_terms_covered": kta_rows[j]["key_terms_covered"],
            "triple_exact": triple_rows[j]["triple_exact"],
            "triple_soft": triple_rows[j]["triple_soft"],
            "bertscore_f1": bs_f1[j],
        }
        records.append(rec)

    metrics_df = pd.DataFrame(records)

    def safe_mean(s):
        s = pd.Series([x for x in s if x is not None and pd.notnull(x)])
        return float(s.mean()) if len(s) else float("nan")

    print("\nüìä Automatic Metrics on Validation:")
    print(f"- Eval loss:           {eval_loss:.4f} | Perplexity: {eval_ppl:.2f}")
    print(f"- BLEU:                {safe_mean(metrics_df['bleu']):.4f}")
    print(f"- ROUGE-L:             {safe_mean(metrics_df['rougeL']):.4f}")
    print(f"- Facet F1 (macro):    {metrics_df['facet_f1_macro'].mean():.4f}")
    print(f"- Key-Term Coverage:   {metrics_df['key_terms_covered'].mean():.4f}")
    print(f"- Triple Exact/Soft:   {metrics_df['triple_exact'].mean():.4f} / {metrics_df['triple_soft'].mean():.4f}")
    if 'bertscore_f1' in metrics_df:
        print(f"- BERTScore (F1):      {safe_mean(metrics_df['bertscore_f1']):.4f}")

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å CSV
    ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
    out_csv = os.path.join(RESULT_DIR, f"metrics_val_{ts}.csv")
    metrics_df.to_csv(out_csv, index=False, encoding="utf-8")
    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå: {out_csv}")

# ====== 7) Preference Testing (Teacher vs Student) + CACHED CSV ======
if not _has("SEED"):
    SEED = 42

# ‡πÄ‡∏ú‡∏∑‡πà‡∏≠ MAIN_PATH ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏°‡∏≤‡∏Å‡πà‡∏≠‡∏ô
MAIN_PATH = globals().get("MAIN_PATH", "/content/drive/MyDrive/demo_finetuning/")
RESULT_DIR = globals().get("RESULT_DIR", os.path.join(MAIN_PATH))
os.makedirs(RESULT_DIR, exist_ok=True)

EVAL_DIR = RESULT_DIR
EVAL_CSV = os.path.join(EVAL_DIR, "evaluateTeacherStudent.csv")

# ‡∏™‡∏ß‡∏¥‡∏ï‡∏ä‡πå‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà (‡∏ó‡∏±‡∏ö cache)
FORCE_RERUN_EVAL = False  # True = ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÅ‡∏°‡πâ‡∏°‡∏µ evaluateTeacherStudent.csv ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
def _safe_mean(series):
    s = pd.Series([x for x in series if pd.notnull(x)])
    return float(s.mean()) if len(s) else float("nan")

# === A) ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå evaluateTeacherStudent.csv ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà ‚Üí ‡πÇ‡∏ä‡∏ß‡πå‡πÅ‡∏•‡∏∞‡∏à‡∏ö ===
if os.path.exists(EVAL_CSV) and not FORCE_RERUN_EVAL:
    print(f"üìÇ ‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå evaluate ‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤: {EVAL_CSV}")
    _prev_eval = pd.read_csv(EVAL_CSV)

    if "winner" in _prev_eval.columns:
        _win_rate = (_prev_eval["winner"] == "Student").mean()
    else:
        _win_rate = float("nan")

    print("\nüìä ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å evaluateTeacherStudent.csv (‡πÅ‡∏Ñ‡∏ä):")
    print(f"- Student win-rate: {_win_rate:.2%}")
    for nm in ["summary_train_loss_last","summary_eval_loss_last","summary_perplexity_eval",
               "summary_bleu_mean","summary_rougeL_mean","summary_facet_f1_macro_mean",
               "summary_key_terms_covered_mean","summary_triple_exact_mean",
               "summary_triple_soft_mean","summary_bertscore_f1_mean"]:
        if nm in _prev_eval.columns:
            print(f"- {nm}: {_safe_mean(_prev_eval[nm]):.4f}")

    print("\nüëÄ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏£‡∏Å ‡πÜ:")
    print(_prev_eval.head().to_string(index=False))

else:
    # === B) ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå (‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡∏£‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà) ‚Üí ‡∏ó‡∏≥ evaluate ‡πÉ‡∏´‡∏°‡πà ===
    N_SAMPLE = min(20, len(df_val))
    sample_val = df_val.sample(N_SAMPLE, random_state=SEED).reset_index(drop=True)

    # Student answers
    student_answers = []
    if _has("model") and _has("tokenizer"):
        model.eval()
        device = "cuda" if torch.cuda.is_available() else "cpu"
        def generate_answer(prompt, max_new_tokens=160):
            inputs = tokenizer(prompt, return_tensors="pt").to(device)
            with torch.no_grad():
                outs = model.generate(**inputs, max_new_tokens=max_new_tokens)
            return tokenizer.decode(outs[0], skip_special_tokens=True)
        for _, row in sample_val.iterrows():
            student_answers.append(generate_answer(build_prompt(row), max_new_tokens=160))
    else:
        print("‚ö†Ô∏è ‡∏Ç‡πâ‡∏≤‡∏° Student answers: ‡πÑ‡∏°‡πà‡∏°‡∏µ model/tokenizer")
        device = "cuda" if torch.cuda.is_available() else "cpu"  # ‡∏Å‡∏±‡∏ô‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô LLM Judge
        student_answers = [""] * len(sample_val)

    # Teacher answers (‡πÄ‡∏£‡∏µ‡∏¢‡∏Å API; ‡∏ñ‡πâ‡∏≤ error ‚Üí ‡πÉ‡∏ä‡πâ reference ‡πÉ‡∏ô df_val)
    teacher_answers = []
    try:
        for _, row in sample_val.iterrows():
            teacher_answers.append(
                client.responses.create(model="gpt-4o-mini", input=build_prompt(row)).output_text
            )
    except Exception as e:
        print(f"‚ö†Ô∏è ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å Teacher ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ: {e}")
        teacher_answers = list(sample_val["output"])

    # ‡πÉ‡∏ä‡πâ rougeL_score ‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏≤‡∏°‡πÑ‡∏ß‡πâ‡πÅ‡∏•‡πâ‡∏ß‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô (‡∏à‡∏∞‡∏Ñ‡∏∑‡∏ô None ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ rouge_scorer)
    rows_compare = []
    for i, row in sample_val.iterrows():
        ref = str(row["output"])
        stu = student_answers[i] if i < len(student_answers) else ""
        tea = teacher_answers[i]

        r_stu = rougeL_score(ref, stu) if 'rougeL_score' in globals() else None
        r_tea = rougeL_score(ref, tea) if 'rougeL_score' in globals() else None
        r_stu = r_stu if (r_stu is not None) else 0.0
        r_tea = r_tea if (r_tea is not None) else 0.0

        winner = "Student" if r_stu > r_tea else ("Teacher" if r_tea > r_stu else "Tie")
        rows_compare.append({
            "id": int(i),
            "rougeL_student": round(float(r_stu), 4),
            "rougeL_teacher": round(float(r_tea), 4),
            "winner": winner
        })

    tbl = pd.DataFrame(rows_compare)
    win_rate = (tbl["winner"] == "Student").mean()

    print("\nüéØ Preference (ROUGE-L-to-Ref) ‚Äî up to 20 prompts")
    print(tbl.to_string(index=False))
    print(f"\nüèÅ Student win-rate: {win_rate:.2%}  "
          f"(Teacher wins {(tbl['winner']=='Teacher').mean():.2%}, "
          f"Ties {(tbl['winner']=='Tie').mean():.2%})")

    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á LLM Judge 1 ‡πÄ‡∏Ñ‡∏™ (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á CSV ‡πÄ‡∏õ‡πá‡∏ô example)
    teacher_answer_example = None
    student_answer_example = None
    judge_reason_example = None
    try:
        eval_prompt = "Translate the mechanism of action of Augmentin 625 Duo Tablet into English and keep it to one sentence."
        if _has("model") and _has("tokenizer"):
            inputs = tokenizer(eval_prompt, return_tensors="pt").to(device)
            with torch.no_grad():
                out = model.generate(**inputs, max_new_tokens=120)
            student_answer_example = tokenizer.decode(out[0], skip_special_tokens=True)

        teacher_answer_example = client.responses.create(
            model="gpt-4o-mini",
            input=eval_prompt
        ).output_text

        judge_prompt = f"""
        You are a fair evaluator. Compare the following responses.

        PROMPT: {eval_prompt}

        [Teacher Answer]
        {teacher_answer_example}

        [Student Answer]
        {student_answer_example}

        Decide which one is better (Teacher or Student) and explain why. And response me in Thai language
        """
        judge = client.responses.create(model="gpt-4o-mini", input=judge_prompt)
        judge_reason_example = judge.output_text

        print("\nüß† Teacher Output:\n", teacher_answer_example)
        print("\nüéì Student Output:\n", student_answer_example)
        print("\n‚öñÔ∏è LLM Judge:\n", judge_reason_example)
    except Exception as e:
        print(f"‚ö†Ô∏è LLM judge skipped: {e}")

    # ----- ‡∏£‡∏ß‡∏°‡∏™‡∏£‡∏∏‡∏õ/‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå metrics ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ prev_df) -----
    bleu_mean = _safe_mean(prev_df['bleu']) if 'prev_df' in globals() and 'bleu' in prev_df else None
    rougeL_mean = _safe_mean(prev_df['rougeL']) if 'prev_df' in globals() and 'rougeL' in prev_df else None
    facet_f1_macro_mean = _safe_mean(prev_df['facet_f1_macro']) if 'prev_df' in globals() and 'facet_f1_macro' in prev_df else None
    key_terms_covered_mean = _safe_mean(prev_df['key_terms_covered']) if 'prev_df' in globals() and 'key_terms_covered' in prev_df else None
    triple_exact_mean = _safe_mean(prev_df['triple_exact']) if 'prev_df' in globals() and 'triple_exact' in prev_df else None
    triple_soft_mean = _safe_mean(prev_df['triple_soft']) if 'prev_df' in globals() and 'triple_soft' in prev_df else None
    bertscore_f1_mean = _safe_mean(prev_df['bertscore_f1']) if 'prev_df' in globals() and 'bertscore_f1' in prev_df else None

    # Perplexity (‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≤‡∏Å eval_loss ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
    try:
        ppl_eval = math.exp(float(eval_loss)) if (eval_loss == eval_loss) else None
    except Exception:
        ppl_eval = None

    # ----- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å evaluateTeacherStudent.csv -----
    summary_cols = {
        "summary_train_loss_last": globals().get("last_train_loss", None),
        "summary_eval_loss_last": globals().get("last_eval_loss", None),
        "summary_perplexity_eval": ppl_eval,
        "summary_bleu_mean": bleu_mean,
        "summary_rougeL_mean": rougeL_mean,
        "summary_facet_f1_macro_mean": facet_f1_macro_mean,
        "summary_key_terms_covered_mean": key_terms_covered_mean,
        "summary_triple_exact_mean": triple_exact_mean,
        "summary_triple_soft_mean": triple_soft_mean,
        "summary_bertscore_f1_mean": bertscore_f1_mean,
        "summary_student_win_rate": float(win_rate),
        "example_teacher_output": teacher_answer_example,
        "example_student_output": student_answer_example,
        "example_judge_reason": judge_reason_example,
    }

    df_save = tbl.copy()
    for k, v in summary_cols.items():
        df_save[k] = v

    df_save.to_csv(EVAL_CSV, index=False, encoding="utf-8")
    print(f"\nüíæ Saved evaluateTeacherStudent.csv ‚Üí {EVAL_CSV}")

    # Preview
    try:
        print("\nüëÄ Preview evaluateTeacherStudent.csv:")
        print(df_save.head().to_string(index=False))
    except Exception:
        pass